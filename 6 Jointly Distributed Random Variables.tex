\documentclass{article}
\usepackage[utf8]{inputenc}
\linespread{1.3}
\title{6 Jointly Distributed Random Variables}
\author{Revisit \emph{A First Course in Probability}}
\date{April 2020}

\usepackage{natbib}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{geometry}
\usepackage{outlines}
\usepackage[T1]{fontenc}
 \geometry{
 a4paper,
 total={170mm,257mm},
 left=20mm,
 top=20mm,
 }
\begin{document}

\maketitle

\section*{Menu}
\begin{outline}[enumerate]
    \1  Joint Distribution Functions 
    \1  Independent RV
    \1  Sums of Independent RVs
        \2 Identically Distributed Uniform Random Variables
        \2 Gamma RVs
        \2 Normal RVs
        \2 Poisson and Binomial RVs
        \2 Geometric RVs
    \1  Conditional Distributions: Discrete Case 
    \1  Conditional Distributions: Continuous Case 
    \1  Order Statistics
    \1  Joint Probability Distribution of Functions of RVs
    \1  Exchangeable RVs
\end{outline}


\section*{6.1 Joint Distribution Functions}
$F(a,b) = P(X \leq a, Y \leq b) -\inf < a, b < \inf$ 
\begin{align*}
F_X(a)  &= P(X \leq a) \\ 
        &= P(X \leq a, Y < \inf) \\ 
        &= P(\lim_{b \rightarrow \inf}(X \leq a, Y \leq b)) \\ 
        &= \lim_{b \rightarrow \inf} P(X \leq a, Y \leq b) \\ 
        &=\lim_{b \rightarrow \inf} F(a,b) \\ 
        &= F(a, \inf)
\end{align*}
$ F_Y(b) = F(\inf, b)$ \underline{\textbf{Marginal distributions of X and Y}} \\ 
All joint prob statements about X and Y can, in theory, be answered in terms of their joint distribution fcn. \\
For instance, joint prob that X is greater than a and Y is greater than b. 
\begin{align*}
P(X>a, Y>b) &= 1- P((X > a, Y > b)^C) \\ 
            &= 1- P((X > a)^C \cup (Y > b)^C) \\
            &= 1 - P((X \leq a) \cup (Y \leq b))  (1.1) \\
            &= 1 - [P(X \leq a) + P(Y \leq b) - P(X \leq a, Y \leq b)]\\
            &= 1 - F_X(a) - F_Y(b) + F(a,b) 
\end{align*}
$P(a_1 < X \leq a_2, b_1 < Y \leq b_2) = F(a_2, b_2) + F(a_1, b_1) - F(a_1, b_2) - F(a_2, b_1)$ (1.2)\\ 
Comment: Makes much more sense when revisit it after two years haha \\ 
In the case when X and Y are both discrete rvs, it is convenient to define the \underline{\textbf{joint prob mass fcn of X and Y}} by \\
$p(x,y) = P(X=x, Y=y)$ \\
The \underline{\textbf{prob mass fcn of X}} can be obtained from p(x,y) by \\
$ p_X(x) = P(X=x) = \sum_{y:p(x,y) > 0}p(x,y)$ \\
Similarly, \\
$ p_Y(y) = \sum_{x:p(x,y) > 0}p(x,y)$\\
Because the individual prob mass fcns of X and Y thus appear in the margin of such a table, they are often referred to as the \underline{\textbf{marginal prob mass fcns}} of X and Y, respectively. \\
We say that X and Y are \underline{\textbf{jointly continuous}} if there exists a fcn f(x,y), defined for all real x and y, having the property that, for every set C of pairs of real numbers (that is, C is a set in the two-dimensional place),\\
$P((X,Y)\in C) = \int_{(x,y)\in C} f(x,y)dx dy $ (1.3)\\ 
The function f(x,y) is called the joint probability density function of X and Y. If A and B are any sets of real numbers, then, by defining $C=[(x,y):x \in A, y \in B]$, we see from Equation (1.3) that \\
$P(X \in A, Y \in B) = \int_{B}\int_{A}f(a,y)dx dy$ (1.4) \\ 
Because \\
$F(a,b) = P(X \in (-\inf, a), Y \in (-\inf, b)) = \int_{-\inf}^{b}\int_{-\inf}^{a}f(x,y)dx dy$ \\ 
it follows, upon differentiation, that \\
$f(a,b) = \frac{\partial^2}{\partial a \partial b}F(a, b)$\\
wherever the partial derivatives are defined. Another interpretation of the joint density function, obtained from Equation (1.4), is \\
Comment: Tbh, from my experience, discrete is much more useful in real life \\
$P(a < X < a + da, b < Y < b + db) = \int_{b}^{b + db} \inf_{a}^{a + da} f(x,y) dx dy$\\
when da and db are small and f(x,y) is continuous at a,b. Hence f(a,b) is the measure of how likely it is that the random vector (X,Y) will be near (a,b). \\
If X and Y are continuous, they are individually continuous, and their prob density fcns can be obtained as follows: 
\begin{align*}
P(X \in A)  &= P(X \in A, Y \in (-\inf, \inf)) \\ 
            &= \int_{A}\int_{-\inf}^{\inf}f(x,y)dy dx \\ 
            &= \int_{A}f_X(x)dx \\ 
where \\
f_X(x)      &= \int_{-\inf}^{\inf}f(x,y)dy
\end{align*}
is thus the prob density fcn of X. Similarly, the prob density fcn of Y is given by \\
$f_Y(y) = \int_{-\inf}^{\inf}f(x,y)dx$\\
Side: I really like this question of $P(X < Y)$ \\ 
The joint density fcn of X and Y is given by \\ 
$ f(x,y) = 2e^{-x}e^{-2y}$ $0< x < \inf, 0 < y < \inf$ and 0 otherwise 
\begin{align*}
P(X,Y)  &= \int_{(x,y): x < y} \int 2e^{-x}e^(-2y)dx dy \\
        &= \int_{0}^{\inf} \int_{0}^{y} 2e^{-x}e^(-2y)dx dy \\
        &= \int_{0}^{\inf} 2e^{-2y}(1-e^{-y})dy \\
        &= \int_{0}^{\inf} 2e^{-2y}dy - \int_{0}^{\inf} 2e^{-3y}dy \\
        &= 1 - \frac{2}{3} \\
        &= \frac{1}{3}
\end{align*}
We can also define joint prob distributions for n rvs in exactly the same manner as we did for n = 2. \\
For instance, the joint cumulative prob distri fcn $F(a_1, a_2, ..., a_n)$ of the n rvs $X_1, X_2, ..., X_n$ is defined by \\
$F(a_1, a_2, ..., a_n) = P(X_1 \leq a_1, X_2 \leq a_2, ..., X_n \leq a_n)$\\
Further, the n rvs are said to be \underline{\textbf{jointly continuous}} if there exists a fcn $f(a_1, a_2, ..., a_n)$, \\
called the \underline{\textbf{joint probability density fcn}}, st, for any set C in n-space, \\
$P((X_1, X_2, ..., X_n)\in C) = \int_{(x_1,...,x_n)\in C} \int ... \int f(x_1,...,x_n)dx_1 dx_2 ... dx_n $ \\ 
In particular, for any n sets of real numbers $A_1, A_2, ... ,A_n$,\\
$P(X_1 \in A_1, X_2 \in A_2, ...,X_n \in A_n) = \int_{A_n} \int_{A_{n-1}} ... \int_{A_1} f(x_1,...,x_n)dx_1 dx_2 ... dx_n $ \\ 
The multinomial distribution\\
One of the most important joint distributions is the multinomial distribution, which arises when a sequence of n independent and identical experiments is performed. Suppose that each experiment can result in any one of r possible outcomes, with respective prob $p_1, p_2, ..., p_r, \sum_{i=1}^r p_i = 1$. If we let $X_i$ denote the number of the n experiments that result in outcome number i, then \\ 
$P(X_1 = n_1, X_2 = n_2, ..., X_r = n_r) = \frac{n!}{n_1!n_2!...n_r!}p_1^{n_1}p_2^{n_2}...p_r^{n_r}$ (1.5) whenever $\sum_{i=1}^r n_i =n.$ \\
Equation (1.5) is verified by noting that any sequence of outcomes for the n experiments that leads to outcome i occurring $n_i$ times for i = 1,2, ...,r will, by the assumed independence of experiments, have prob $p_i^{n_1}p_2^{n_2}...p_r^{n_r}$ of occurring. Because there are $\frac{n!}{n_1!n_2!...n_r!}$ such sequences of outcomes (there are $\frac{n!}{n_1!n_2!...n_r!}$ different permutations of n things of which $n_1$ are alike, ..., $n_r$ are alike), Equation (1.5) is established.\\
Comment: Well said! \\
The joint distribution whose joint probability mass fcn is specified by Equation (1.5) is called the \\
\underline{\textbf{multinomial distri}}. Note that when r = 2, the multinomial reduces to the binomial distri. \\
Note also that any sum of a fixed set of the $X_i$'s will have a binomial distri. That is, if N $\subset {1,2,...,r}$, then $\sum_{i \in N}X_i$ will be a binomial rv with parameters n and $p=\sum_{i \in N} p_i$. This follows because $\sum_{i \in N}X_i$ represents the number of the n experiments whose outcome is in N, and each experiment will independently have such an outcome with probability $p=\sum_{i \in N} p_i$. 
\section*{6.2 Independent RVs}
The rvs X and Y are said to be \underline{\textbf{independent}} if, for any two sets of real numbers A and B, \\
$P(X \in A, Y \in B) = P(X \in A)P(Y \in B)$ (2.1) \\ 
In other words, X and Y are are independent if, for all A and B, the events $E_A = {X \in A}$ and $E_B = {Y \in B}$ are independent. \\
In can be shown by using the three axioms of prob that Equation(2.1) will follow iff, for all a, b, \\
$P(X \leq a, Y \leq b) = P(X \leq a)P(Y \leq b)$\\
Hence, in terms of the joint distribution fcn F of X and Y, X and Y are indep if\\
$F(a,b) = F_X(a)F_Y(b)$ for all a,b \\
When X and Y are discrete rvs, the condition of indep (2,1) is equivalent to \\
$p(x,y) = p_X(x)p_Y(y)$ for all x,y (2.2)\\
In the jointly continuous case, the condition of indep is equivalent to \\
$f(x,y) = f_X(x)f_Y(y)$ for all x,y \\
Thus, loosely speaking, X and Y are indep if knowing the value of one does not change the distri of the other. Rvs that are not indep are said to be dependent. \\
Eg 2c. A man and a woman decide to meet at a certain location. If each of them indep arrives at a time uniformly distributed between 12 noon and 1 PM, find the probability that the first to arrive has to wait longer than 10 minutes. [Very nice; really like this question!] \\
Sol: Let X and Y denote, the time past 12 the the man and the woman arrive, then X and Y are indep rv, each of which is uniformly distributed over (0,60). The desired prob, P(X + 10 < Y) + P(Y + 10 < X), which, by symmetry, equals 2P(X + 10 < Y), is obtained as follows: 
\begin{align*}
    2P(X + 10 < Y)  &= 2 \int_{x + 10 < y} \int f(x,y)dxdy \\
                    &= 2 \int_{x + 10 < y} \int f_X(x)f_Y(y)dxdy \\
                    &= 2 \int_{10}^{60} \int_{0}^{y-10} (\frac{1}{60})^2 dxdy \\
                    &= \frac{2}{60^2}\int_{10}^{60}(y-10)dy \\ 
                    &= \frac{25}{36}
\end{align*}
Eg 2b (2.3) - (2.8) \\
Suppose that the number of people who enter a post office on a given day is a Poisson rv with parameter $\lambda$. Show that if each person who enters the post office is a male with prob p and a female with prob 1-p, then the number of males and females entering the post office are indep Poisson rvs with respective parameters $\lambda p$ and $\lambda (1-p)$ 
\begin{align*}
    P(X = i, Y =j)  &= P(X = i, Y =j | X + Y = i + j)P(X + Y = i + j) \\              &+ P(X = i, Y =j | X + Y \neq i + j)P(X + Y \neq i + j) \\ 
                    &= P(X = i, Y =j | X + Y = i + j)P(X + Y = i + j) (2.3) \\
                    &= e^{-\lambda}\frac{\lambda^{i+j}}{(i+j)!} (2.4) \\
     P(X = i, Y =j | X + Y = i + j) &= \binom(i+j,i)p^i(1-p)^j (2.5) \\
     P(X = i, Y =j) &= \frac{e^{-\lambda p}(\lambda p)^i}{i!}e^{-\lambda (1-p)}\frac{[\lambda(1-p)]^j}{j!} (2.6) \\
     P(X=i)         &= e^{-\lambda p}\frac{(\lambda p)^i}{i!} (2.7) \\
     P(Y=j)         &= e^{-\lambda p}\frac{(\lambda p)^j}{j!} (2.8)
\end{align*}
Eg 2e Characterization of the normal distribution \\
Let X and Y denote the horizontal and vertical miss distance with a bullet is fired at a target, and assume that \\
1. X and Y are indep continuous rv having differentiable density fcns. \\
2. The joint density $f(x,y)=f_X(x)f_Y(y)$ of X and Y depends on (x,y) only through $x^2 + y^2$.\\
It is a rather interesting fact that assumptions 1 and 2 imply that X and Y are normally distributed rvs. Proof:\\ 
$f(x,y)=f_X(x)f_Y(y)=2x g(x^2 + y^2)(2.9)$ \\
for some fcn g. Differentiating Eqn(2.9) wrt x yields \\
$f'_X(x)f_Y(y)=2x g'(x^2 + y^2) (2.10)$\\
Dividing Eqn(2.10) by Eqn(2.9) gives \\
$\frac{f'_X(x)}{f_X(x} = \frac{2x g'(x^2 + y^2)}{g'(x^2 + y^2)}$\\
or $\frac{f'_X(x)}{2x f_X(x} = \frac{g'(x^2 + y^2)}{g'(x^2 + y^2)}(2.11)$\\
Because the value of the lhs depends only on x, whereas the value of the rhd depends on $x^2 + y^2$, it follows that the lhs must be the same for all x. \\
Hence,\\
$\frac{f'_X(x)}{x f_X(x)}=c$ or $\frac{d}{dx}(log f_X(x))=c x$\\
which implies, upon integration of both sides, that \\
$log f_X(x)= a + \frac{cx^2}{2}$ or $f_X(x)=k e^{c x^2/2}$\\
Since $\int_{-\inf}^{\inf}f_X(x)dx =1$, it follows that c is necessarily negative, and we may write c = -1/$\sigma^2$. Thus, \\
$f_X(x)=k e^{-x^2/2\sigma^2}$\\
That is, X is a normal rv with paras $\mu$ = 0 and $\sigma^2$. A similar argument can be applied to $f_Y(y) to show that$ \\ 
$f_Y(x)=\frac{1}{\sqrt{2\pi}\hat{\sigma}}e^{-x^2/2\hat{\sigma}^2}$\\
Furthermore, it follows from assumption 2 that $\sigma^2 = \hat{\sigma}^2$  \\
A necessary and sufficient condition for the rv X and  Y to be indep is for their joint prob density fcn(or joint prob mass fcn in the discrete case) f(x,y) to factor into two terms, one depending only on x and the other depending only on y. \\
Proposition 2.1. The continuous(discrete) rvs X and Y are indep iff their joint prob density(mass) fcn can be expressed as \\
$f_{X,Y}(x,y) = h(x)g(y) -\inf < x < \inf, -\inf<y<\inf$ \\
Finally, we say that an infinite collection of rvs is indep if every finite subcollection of them is indep.
\section*{6.3 Sums of Independent RVs}
\begin{align*}
    F_{X+Y} &= P(X + Y \leq a) \\
            &= \int_{x+y \leq a}\int f_X(x)f_Y(y)dxdy (3.1) \\
            &= \int_{-\inf}^{\inf}\int_{a-y}^{\inf}f_X(x)dx f_Y(y)dy \\
            &= \int_{-\inf}^{\inf}F_X(a-y)f_Y(y)dy
\end{align*}
The cumulative distribution function $F_{X+Y}$ is called the convolution of the distributions $F_X$ and $F_Y$ (the cumulative distribution functions of X and Y, respectively). \\
By differentiating Equation(3.1), we find that the probability density function $f_{X+Y}$ of X+Y is given by 
\begin{align*}
    f_{X+Y}(a)  &= \frac{d}{da}\int_{-\inf}^{\inf}F_X(a-y)f_Y(y)dy \\
                &= \int_{-\inf}^{\inf}\frac{d}{da}F_X(a-y)f_Y(y)dy (3.2)\\
                &= \int_{-\inf}^{\inf}f_X(a-y)f_Y(y)dy  
\end{align*}

\subsection*{6.3.1 Identically Distributed Uniform Random Variables}
Eg 3a Sum of two indep uniform rvs \\
\begin{align*}
    f_X(a) = f_Y(a) = \begin{cases}
                        1, & \text{ 0 < a < 1} \\
                        0, & \text{otherwise}
                      \end{cases}
\end{align*}
we obtain \\
$f_{X+Y}(a) = \int_{0}^1f_X(a-y)dy$\\
For $0 \leq a \leq 1$, this yields \\
$f_{X+Y}(a) = \int_{0}^a dy = a$\\
For $1 \leq a \leq 2$, we get \\
$f_{X+Y}(a) = \int_{a-1}^1 dy = 2 - a$\\
Hence 
\begin{align*}
    f_{X+Y}(a)  = f_Y(a) = \begin{cases}
                        a, & \text{ 0 $\leq$ a $\leq$ 1} \\
                        2 - a, & \text{ 1 < a < 2} \\
                        0, & \text{otherwise}
                      \end{cases}
\end{align*}
Because of the shape of its density function, the rv X + Y is said to have a triangular distribution. \\
Now, suppose that $X_1, X_2, ..., X_n$ are indep uniform (0,1) rvs, and let \\
$F_n(x) = P(X_1 + X_2 + ... + X_n) \leq x$ \\
Whereas a general formula for $F_n(x)$ is messy, it has a particularly nice form when $x \leq 1$. Indeed, we now use mathematical induction to prove that \\ 
$F_n(x) = x^n/n!, 0 \leq x \leq 1$ \\ 
For an interesting application of the preceding formula, [Comment: I attest, indeed interesting :)] \\ 
let us use it to determine the expected number of indep uniform (0,1) rvs that need to be summed to exceed 1. \\
That is, with $X_1, X_2, ...$ being independent uniform (0,1) rvs, want to determine E[N], where $N = min{n: X_1 + ...  X_n >1}$\\
Noting that N is greater than n > iff $X_1 + ...  X_n \leq 1$, we see that \\
$ P(N>n) = F_n(1) = 1/n!, n>0$ \\
Because \\
$ P(N>0) = 1 = 1/0!$ \\
we see that, for n > 0, \\
$ P(N = n) = P(N > n-1) - P(N > n) = \frac{1}{(n-1)!} - \frac{1}{n!} = \frac{n-1}{n!}$ \\ 
Therefore, 
\begin{align*}
    E[N]&= \sum_{n=1}^{\inf}\frac{n(n-1)}{n!} \\
        &= \sum_{n=2}^{\inf}\frac{1}{(n-2)!} \\
        &= e
\end{align*}
That is, the mean number of indep uniform (0,1) rvs that must be summed for the sum to exceed 1 = e. 

\subsection*{6.3.2 Gamma Random Variables}
A gamma rv has a density of the form $f(y) = \frac{\lambda e^{-\lambda y}(\lambda y)^{t-1}}{\Gamma(t)} 0 < y < \inf$
\\
Proposition 3.1 If X and Y are independent gamma rvs with variables with respective parameters $(s, \lambda)$ and $(t, \lambda)$, then X + Y is a gamma rv with parameters (s+t, $\lambda$). \\
Eg 3b Let $X_1, X_2, ..., X_n$ be n independent exponential rvs, each having parameter $\lambda$. Then, since an exponential rv with parameter $\lambda$ is the same as a gamma rv with para (1, $\lambda$), it follows from Proposition 3.1 that $X_1 + X_2 + ... + X_n$ is a gamma rv with parameters (n, $\lambda$)\\
If $Z_1, Z_2, ..., Z_n$ are independent standard normal rvs, then $Y \equiv \sum_{i=1}^n Z_i^2$ is said to have the chi-squared (sometimes seen as $\chi^2$) distribution with n degrees of freedom. Let us compute the density function of Y. In the following equation. when n = 1, Y = $Z_1^2$, we see that its probability density function is given by 
\begin{align*}
    f_{Z^2}(y) &= \frac{1}{2\sqrt{y}}[f_Z(\sqrt{y}) + f_Z(-\sqrt{y})]\\
        &= \frac{1}{2\sqrt{y}} \frac{2}{\sqrt{2\pi}} e^{-y/2} \\
        &= \frac{\frac{1}{2}e^{-y/2}(y/2)^{1/2-1}}{\sqrt{\pi}}
\end{align*}

\subsection*{6.3.3 Normal Random Variables}
Proposition 3.2 If $X_i, i = 1, ..., n$ are independent rvs that are normally distributed with respective parameters $\mu_i, \sigma_i^2, i = 1, ..., n$, then $\sum_{i=1}^n X_i$ is normally distributed with parameters $\sum_{i=1}^n \mu_i$ and $\sum_{i=1}^n \sigma_i^2$
\\
Poisson Paradigm. Consider n events, with $p_i$ equal to the probability that event i occurs, i = 1,...,n. If all the $p_i$ are "small" and the trials are either independent or at most "weakly dependent," then the number of these events that occur approximately has a Poisson distribution with mean $\sum_{i=1}^n p_i$\\
Example 7d Length of the longest run \\

\subsection*{6.3.4 Poisson and Binomial Random Variables}


\subsection*{6.3.5 Geometric Random Variables}

\subsection*{6.4 Conditional Distributions: Discrete Case}

\subsection*{6.4 Conditional Distributions: Continuous Case}

\subsection*{6.6 Order Statistics} 

\section*{Exercises}
4. Suppose 3 balls are chosen without replacement from an urn consisting of 5 white and 8 red balls. The white balls are numbered and let $Y_i$ equal 1 if the i-th white ball is selected and 0 ow. Find the joint prob mass function of \\
(a) $Y_1, Y_2$ \\
(b) $Y_1, Y_2, Y_3$ \\
Solution: \\
(a) p(0, 0) = (10/13)(9/12) = 15/26 \\ 
p(0, 1) = p(1, 0) = (10/13)(3/12) = 5/26 \\ 
p(1, 1) = (3/13)(2/12) = 1/26 \\ 
(b) p(0, 0, 0) = (10/13)(9/12)(8/11) = 60/143 \\ 
p(0, 0, 1) = p(0, 1, 0) = p(1, 0, 0) = (10/13)(9/12)(3/11) = 45/286 \\ 
p(i, j, k) = (3/13)(2/12)(10/11) = 5/143 if i + j + k = 2 \\ 
p(1, 1, 1) = (3/13)(2/12)(1/11) = 1/286 \\
27. If $X_1$ and $X_2$ are independent exponential rvs with respective parameters $\lambda_1$ and $\lambda_2$, find the distribution of Z = $X_1/X_2$. Also compute $P(X_1<X_2)$

% \begin{figure}[h!]
% \centering
% \includegraphics[scale=1.7]{universe}
% \caption{The Universe}
% \label{fig:universe}
% \end{figure}
% \citep{adams1995hitchhiker}
\bibliographystyle{plain}
\bibliography{references}
\end{document}

